{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"中文bert情感分类 demo-chinese-text-binary-classification-with-bert.ipynb","provenance":[{"file_id":"https://github.com/wshuyi/demo-chinese-text-binary-classification-with-bert/blob/master/demo_chinese_text_binary_classification_with_bert.ipynb","timestamp":1582423539736}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"GBbAMqZ6IZuw","colab_type":"text"},"source":["base code borrowed from [this Google Colab Notebook](https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb).\n","\n","Refactored by [Shuyi Wang](https://www.linkedin.com/in/shuyi-wang-b3955026/)\n","\n","Please refer to [this Medium Article](https://medium.com/@wshuyi/how-to-do-text-binary-classification-with-bert-f1348a25d905) for the tutorial on how to classify English text data.\n","\n"]},{"cell_type":"code","metadata":{"id":"jviywGyWyKsA","colab_type":"code","trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":145},"outputId":"ca4b94de-b669-49a8-8d6d-42883f111595","executionInfo":{"status":"ok","timestamp":1582423943286,"user_tz":-480,"elapsed":7597,"user":{"displayName":"zhe zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC0NRKj7554RjE94pOQzifJSQAmHjNZwEUmnEjm=s64","userId":"13769432557712125381"}}},"source":["!pip install bert-tensorflow"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting bert-tensorflow\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n","\r\u001b[K     |████▉                           | 10kB 24.1MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 20kB 30.9MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 30kB 36.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 40kB 40.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 51kB 39.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 61kB 41.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 7.9MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n","Installing collected packages: bert-tensorflow\n","Successfully installed bert-tensorflow-1.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hsZvic2YxnTz","colab_type":"code","trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":121},"outputId":"c31a23f6-f79f-49f7-9417-63928bcfeb84","executionInfo":{"status":"ok","timestamp":1582423954606,"user_tz":-480,"elapsed":2919,"user":{"displayName":"zhe zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC0NRKj7554RjE94pOQzifJSQAmHjNZwEUmnEjm=s64","userId":"13769432557712125381"}}},"source":["import pandas as pd\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import pickle\n","import bert\n","from bert import run_classifier\n","from bert import optimization\n","from bert import tokenization"],"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NZM71PjIOF_I","colab":{}},"source":["def pretty_print(result):\n","    df = pd.DataFrame([result]).T\n","    df.columns = [\"values\"]\n","    return df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"ZCYOL8HbIZu2","colab_type":"code","colab":{}},"source":["def create_tokenizer_from_hub_module(bert_model_hub):\n","  \"\"\"\n","  Get the vocab file and casing info from the Hub module.\n","  https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\n","  大写转小写，这个model TODO 这一步其实不需要吧\n","  \"\"\"\n","  with tf.Graph().as_default():\n","    bert_module = hub.Module(bert_model_hub)\n","    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n","    with tf.Session() as sess:\n","      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n","                                            tokenization_info[\"do_lower_case\"]])\n","      \n","  return bert.tokenization.FullTokenizer(\n","      vocab_file=vocab_file, do_lower_case=do_lower_case)\n","\n","def make_features(dataset, label_list, MAX_SEQ_LENGTH, tokenizer, DATA_COLUMN, LABEL_COLUMN):\n","    # run_classifier.InputExample 处理成bert可读形式\n","    input_example = dataset.apply(lambda x: run_classifier.InputExample(guid=None, text_a = x[DATA_COLUMN], text_b = None, label = x[LABEL_COLUMN]), axis = 1)\n","    # input example 转 feature TODO tokenizer 可以不要\n","    features = run_classifier.convert_examples_to_features(input_example, label_list, MAX_SEQ_LENGTH, tokenizer)\n","    return features\n","\n","def create_model(bert_model_hub, is_predicting, input_ids, input_mask, segment_ids, labels,\n","                 num_labels):\n","  \"\"\"Creates a classification model.\"\"\"\n","\n","  bert_module = hub.Module(\n","      bert_model_hub,\n","      trainable=True)\n","  bert_inputs = dict(\n","      input_ids=input_ids,\n","      input_mask=input_mask,\n","      segment_ids=segment_ids)\n","  bert_outputs = bert_module(\n","      inputs=bert_inputs,\n","      signature=\"tokens\",\n","      as_dict=True)\n","\n","  # Use \"pooled_output\" for classification tasks on an entire sentence.\n","  # Use \"sequence_outputs\" for token-level output.\n","  output_layer = bert_outputs[\"pooled_output\"]\n","\n","  hidden_size = output_layer.shape[-1].value\n","\n","  # Create our own layer to tune for politeness data.\n","  output_weights = tf.get_variable(\n","      \"output_weights\", [num_labels, hidden_size],\n","      initializer=tf.truncated_normal_initializer(stddev=0.02))\n","\n","  output_bias = tf.get_variable(\n","      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n","\n","  with tf.variable_scope(\"loss\"):\n","\n","    # Dropout helps prevent overfitting\n","    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n","\n","    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n","    logits = tf.nn.bias_add(logits, output_bias)\n","    log_probs = tf.nn.log_softmax(logits, axis=-1)\n","\n","    # Convert labels into one-hot encoding\n","    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n","\n","    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n","    # If we're predicting, we want predicted labels and the probabiltiies.\n","    if is_predicting:\n","      return (predicted_labels, log_probs)\n","\n","    # If we're train/eval, compute loss between predicted and actual label\n","    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n","    loss = tf.reduce_mean(per_example_loss)\n","    return (loss, predicted_labels, log_probs)\n","\n","# model_fn_builder actually creates our model function\n","# using the passed parameters for num_labels, learning_rate, etc.\n","def model_fn_builder(bert_model_hub, num_labels, learning_rate, num_train_steps,\n","                     num_warmup_steps):\n","  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n","  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n","    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n","\n","    input_ids = features[\"input_ids\"]\n","    input_mask = features[\"input_mask\"]\n","    segment_ids = features[\"segment_ids\"]\n","    label_ids = features[\"label_ids\"]\n","\n","    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n","    \n","    # TRAIN and EVAL\n","    if not is_predicting:\n","\n","      (loss, predicted_labels, log_probs) = create_model(\n","        bert_model_hub, is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n","\n","      train_op = bert.optimization.create_optimizer(\n","          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n","\n","      # Calculate evaluation metrics. \n","      def metric_fn(label_ids, predicted_labels):\n","        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n","        f1_score = tf.contrib.metrics.f1_score(\n","            label_ids,\n","            predicted_labels)\n","        auc = tf.metrics.auc(\n","            label_ids,\n","            predicted_labels)\n","        recall = tf.metrics.recall(\n","            label_ids,\n","            predicted_labels)\n","        precision = tf.metrics.precision(\n","            label_ids,\n","            predicted_labels) \n","        true_pos = tf.metrics.true_positives(\n","            label_ids,\n","            predicted_labels)\n","        true_neg = tf.metrics.true_negatives(\n","            label_ids,\n","            predicted_labels)   \n","        false_pos = tf.metrics.false_positives(\n","            label_ids,\n","            predicted_labels)  \n","        false_neg = tf.metrics.false_negatives(\n","            label_ids,\n","            predicted_labels)\n","        return {\n","            \"eval_accuracy\": accuracy,\n","            \"f1_score\": f1_score,\n","            \"auc\": auc,\n","            \"precision\": precision,\n","            \"recall\": recall,\n","            \"true_positives\": true_pos,\n","            \"true_negatives\": true_neg,\n","            \"false_positives\": false_pos,\n","            \"false_negatives\": false_neg\n","        }\n","\n","      eval_metrics = metric_fn(label_ids, predicted_labels)\n","\n","      if mode == tf.estimator.ModeKeys.TRAIN:\n","        return tf.estimator.EstimatorSpec(mode=mode,\n","          loss=loss,\n","          train_op=train_op)\n","      else:\n","          return tf.estimator.EstimatorSpec(mode=mode,\n","            loss=loss,\n","            eval_metric_ops=eval_metrics)\n","    else:\n","      (predicted_labels, log_probs) = create_model(\n","        bert_model_hub, is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n","\n","      predictions = {\n","          'probabilities': log_probs,\n","          'labels': predicted_labels\n","      }\n","      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n","\n","  # Return the actual model function in the closure\n","  return model_fn\n","\n","def estimator_builder(bert_model_hub, OUTPUT_DIR, SAVE_SUMMARY_STEPS, SAVE_CHECKPOINTS_STEPS, label_list, LEARNING_RATE, num_train_steps, num_warmup_steps, BATCH_SIZE):\n","\n","    # Specify outpit directory and number of checkpoint steps to save\n","    run_config = tf.estimator.RunConfig(\n","        model_dir=OUTPUT_DIR,\n","        save_summary_steps=SAVE_SUMMARY_STEPS,\n","        save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n","\n","    model_fn = model_fn_builder(\n","      bert_model_hub = bert_model_hub,\n","      num_labels=len(label_list),\n","      learning_rate=LEARNING_RATE,\n","      num_train_steps=num_train_steps,\n","      num_warmup_steps=num_warmup_steps)\n","\n","    estimator = tf.estimator.Estimator(\n","      model_fn=model_fn,\n","      config=run_config,\n","      params={\"batch_size\": BATCH_SIZE})\n","    return estimator, model_fn, run_config\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IuMOGwFui4it","colab_type":"code","trusted":true,"colab":{}},"source":["def run_on_dfs(train, test, DATA_COLUMN, LABEL_COLUMN, \n","               MAX_SEQ_LENGTH = 128,\n","              BATCH_SIZE = 32,\n","              LEARNING_RATE = 2e-5,\n","              NUM_TRAIN_EPOCHS = 3.0,\n","              WARMUP_PROPORTION = 0.1,\n","              SAVE_SUMMARY_STEPS = 100,\n","               SAVE_CHECKPOINTS_STEPS = 10000,\n","              bert_model_hub = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"):\n","\n","    label_list = train[LABEL_COLUMN].unique().tolist()\n","    \n","    tokenizer = create_tokenizer_from_hub_module(bert_model_hub)\n","\n","    train_features = make_features(train, label_list, MAX_SEQ_LENGTH, tokenizer, DATA_COLUMN, LABEL_COLUMN)\n","    test_features = make_features(test, label_list, MAX_SEQ_LENGTH, tokenizer, DATA_COLUMN, LABEL_COLUMN)\n","\n","    num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n","    num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n","\n","    estimator, model_fn, run_config = estimator_builder(\n","                                  bert_model_hub, \n","                                  OUTPUT_DIR, \n","                                  SAVE_SUMMARY_STEPS, \n","                                  SAVE_CHECKPOINTS_STEPS, \n","                                  label_list, \n","                                  LEARNING_RATE, \n","                                  num_train_steps, \n","                                  num_warmup_steps, \n","                                  BATCH_SIZE)\n","\n","    train_input_fn = bert.run_classifier.input_fn_builder(\n","        features=train_features,\n","        seq_length=MAX_SEQ_LENGTH,\n","        is_training=True,\n","        drop_remainder=False)\n","\n","    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n","\n","    test_input_fn = run_classifier.input_fn_builder(\n","        features=test_features,\n","        seq_length=MAX_SEQ_LENGTH,\n","        is_training=False,\n","        drop_remainder=False)\n","\n","    result_dict = estimator.evaluate(input_fn=test_input_fn, steps=None)\n","    return result_dict, estimator\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"wpUHTA8LIZu_","colab_type":"code","colab":{}},"source":["import random\n","random.seed(10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"qmP8MBvjIZvB","colab_type":"code","colab":{}},"source":["OUTPUT_DIR = 'output'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UV3ExI2JJMhd","colab_type":"text"},"source":["----- you just need to focus from here ------"]},{"cell_type":"markdown","metadata":{"id":"XSUjVoFtJVEO","colab_type":"text"},"source":["## Get your data"]},{"cell_type":"code","metadata":{"id":"pe6HHONmOwEs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":305},"outputId":"3f3630c2-0b55-42fc-f9e2-23aa59e6a5a8","executionInfo":{"status":"ok","timestamp":1582424167625,"user_tz":-480,"elapsed":5594,"user":{"displayName":"zhe zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC0NRKj7554RjE94pOQzifJSQAmHjNZwEUmnEjm=s64","userId":"13769432557712125381"}}},"source":["!wget https://github.com/wshuyi/demo-chinese-text-binary-classification-with-bert/raw/master/dianping_train_test.pickle"],"execution_count":8,"outputs":[{"output_type":"stream","text":["--2020-02-23 02:16:04--  https://github.com/wshuyi/demo-chinese-text-binary-classification-with-bert/raw/master/dianping_train_test.pickle\n","Resolving github.com (github.com)... 52.74.223.119\n","Connecting to github.com (github.com)|52.74.223.119|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/wshuyi/demo-chinese-text-binary-classification-with-bert/master/dianping_train_test.pickle [following]\n","--2020-02-23 02:16:05--  https://raw.githubusercontent.com/wshuyi/demo-chinese-text-binary-classification-with-bert/master/dianping_train_test.pickle\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 561978 (549K) [application/octet-stream]\n","Saving to: ‘dianping_train_test.pickle’\n","\n","\r          dianping_   0%[                    ]       0  --.-KB/s               \rdianping_train_test 100%[===================>] 548.81K  --.-KB/s    in 0.01s   \n","\n","2020-02-23 02:16:06 (39.5 MB/s) - ‘dianping_train_test.pickle’ saved [561978/561978]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"trusted":true,"id":"XFBwDlmnIZvD","colab_type":"code","colab":{}},"source":["with open(\"dianping_train_test.pickle\", 'rb') as f:\n","    train, test = pickle.load(f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"Tju0c4dqIZvK","colab_type":"code","colab":{}},"source":["train = train.sample(len(train))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p9BAxyKhKirc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":202},"outputId":"8f64e174-ef57-4eaf-d659-a708c2c5e085","executionInfo":{"status":"ok","timestamp":1582424220368,"user_tz":-480,"elapsed":1747,"user":{"displayName":"zhe zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC0NRKj7554RjE94pOQzifJSQAmHjNZwEUmnEjm=s64","userId":"13769432557712125381"}}},"source":["train.head()"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>comment</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>286</th>\n","      <td>味道一般 不怎么好吃 该给的玻璃杯也不给了  一说有没有 一张臭脸摆出来</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>770</th>\n","      <td>点了一个手撕饼，一个烟熏骨，一个口味菜花，还有酸汤肥牛，前面三个都不错，分量也很足，两个人都...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>753</th>\n","      <td>两个人5点半拿号，等到将近7:30吃上，点了熏排骨，月牙骨酸菜，锅包肉，酱油炒饭，味道差强人...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1188</th>\n","      <td>传说很火的店 5点到了 足足等了一个小时 没敢动地方 里面有点像南京大排档 有唱曲的 服务员...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>151</th>\n","      <td>北李家的菜不可靠，不放心！好几次吃完都会胃疼，这次朋友们都反映饭后恶心，还有人夜里去了医院，...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                comment  sentiment\n","286                味道一般 不怎么好吃 该给的玻璃杯也不给了  一说有没有 一张臭脸摆出来          0\n","770   点了一个手撕饼，一个烟熏骨，一个口味菜花，还有酸汤肥牛，前面三个都不错，分量也很足，两个人都...          1\n","753   两个人5点半拿号，等到将近7:30吃上，点了熏排骨，月牙骨酸菜，锅包肉，酱油炒饭，味道差强人...          0\n","1188  传说很火的店 5点到了 足足等了一个小时 没敢动地方 里面有点像南京大排档 有唱曲的 服务员...          1\n","151   北李家的菜不可靠，不放心！好几次吃完都会胃疼，这次朋友们都反映饭后恶心，还有人夜里去了医院，...          0"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"hM8M7k0gKk5H","colab_type":"code","colab":{}},"source":["myparam = {\n","        \"DATA_COLUMN\": \"comment\",\n","        \"LABEL_COLUMN\": \"sentiment\",\n","        \"LEARNING_RATE\": 2e-5,\n","        \"NUM_TRAIN_EPOCHS\":3,\n","        \"bert_model_hub\":\"https://tfhub.dev/google/bert_chinese_L-12_H-768_A-12/1\"\n","    }"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Dg2apeXpMqG-","colab":{"base_uri":"https://localhost:8080/","height":613},"outputId":"f61c0569-cd31-4c33-c1c9-94c1c4576b00","executionInfo":{"status":"ok","timestamp":1582424518124,"user_tz":-480,"elapsed":68846,"user":{"displayName":"zhe zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC0NRKj7554RjE94pOQzifJSQAmHjNZwEUmnEjm=s64","userId":"13769432557712125381"}}},"source":["result, estimator = run_on_dfs(train, test, **myparam)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.04357\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.04357\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.16441922, step = 100 (95.827 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.16441922, step = 100 (95.827 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 150 into output/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 150 into output/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Loss for final step: 0.011572415.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Loss for final step: 0.011572415.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n","/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Starting evaluation at 2020-02-23T02:21:46Z\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Starting evaluation at 2020-02-23T02:21:46Z\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from output/model.ckpt-150\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from output/model.ckpt-150\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished evaluation at 2020-02-23-02:21:56\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Finished evaluation at 2020-02-23-02:21:56\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving dict for global step 150: auc = 0.8980811, eval_accuracy = 0.8975, f1_score = 0.9002432, false_negatives = 24.0, false_positives = 17.0, global_step = 150, loss = 0.37562764, precision = 0.9158416, recall = 0.8851675, true_negatives = 174.0, true_positives = 185.0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving dict for global step 150: auc = 0.8980811, eval_accuracy = 0.8975, f1_score = 0.9002432, false_negatives = 24.0, false_positives = 17.0, global_step = 150, loss = 0.37562764, precision = 0.9158416, recall = 0.8851675, true_negatives = 174.0, true_positives = 185.0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving 'checkpoint_path' summary for global step 150: output/model.ckpt-150\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving 'checkpoint_path' summary for global step 150: output/model.ckpt-150\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"YLOvtveTMqwp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":386},"outputId":"e6de1d9a-cfa6-4055-8f0c-ad5a706d36d6","executionInfo":{"status":"ok","timestamp":1582425161689,"user_tz":-480,"elapsed":1792,"user":{"displayName":"zhe zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC0NRKj7554RjE94pOQzifJSQAmHjNZwEUmnEjm=s64","userId":"13769432557712125381"}}},"source":["pretty_print(result)"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>values</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>auc</th>\n","      <td>0.898081</td>\n","    </tr>\n","    <tr>\n","      <th>eval_accuracy</th>\n","      <td>0.897500</td>\n","    </tr>\n","    <tr>\n","      <th>f1_score</th>\n","      <td>0.900243</td>\n","    </tr>\n","    <tr>\n","      <th>false_negatives</th>\n","      <td>24.000000</td>\n","    </tr>\n","    <tr>\n","      <th>false_positives</th>\n","      <td>17.000000</td>\n","    </tr>\n","    <tr>\n","      <th>loss</th>\n","      <td>0.375628</td>\n","    </tr>\n","    <tr>\n","      <th>precision</th>\n","      <td>0.915842</td>\n","    </tr>\n","    <tr>\n","      <th>recall</th>\n","      <td>0.885167</td>\n","    </tr>\n","    <tr>\n","      <th>true_negatives</th>\n","      <td>174.000000</td>\n","    </tr>\n","    <tr>\n","      <th>true_positives</th>\n","      <td>185.000000</td>\n","    </tr>\n","    <tr>\n","      <th>global_step</th>\n","      <td>150.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                     values\n","auc                0.898081\n","eval_accuracy      0.897500\n","f1_score           0.900243\n","false_negatives   24.000000\n","false_positives   17.000000\n","loss               0.375628\n","precision          0.915842\n","recall             0.885167\n","true_negatives   174.000000\n","true_positives   185.000000\n","global_step      150.000000"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"D0OPTDcGMsJw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"3fc30ae1-47e5-432c-9c8c-5f13c1839793","executionInfo":{"status":"ok","timestamp":1582426657687,"user_tz":-480,"elapsed":8956,"user":{"displayName":"zhe zhu","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC0NRKj7554RjE94pOQzifJSQAmHjNZwEUmnEjm=s64","userId":"13769432557712125381"}}},"source":["def predict(train, test, DATA_COLUMN, LABEL_COLUMN, \n","               MAX_SEQ_LENGTH = 128,            \n","              bert_model_hub = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"):\n","  label_list = train[LABEL_COLUMN].unique().tolist()\n","  tokenizer = create_tokenizer_from_hub_module(bert_model_hub)\n","  test_features = make_features(test, label_list, MAX_SEQ_LENGTH, tokenizer, DATA_COLUMN, LABEL_COLUMN)\n","\n","  # print(\"label_list\", label_list)  # 两类\n","  test_input_fn = run_classifier.input_fn_builder(\n","        features=test_features,\n","        seq_length=MAX_SEQ_LENGTH,\n","        is_training=False,\n","        drop_remainder=False)\n","  \n","  predict_results = estimator.predict(\n","      input_fn=test_input_fn)\n","      \n","  print(\"Predictions on test file\")\n","  for prediction in predict_results: # 预测结果\n","    print(prediction)\n","\n","\n","myparam2 = {\n","        \"DATA_COLUMN\": \"comment\",\n","        \"LABEL_COLUMN\": \"sentiment\",\n","        \"bert_model_hub\":\"https://tfhub.dev/google/bert_chinese_L-12_H-768_A-12/1\"\n","    }\n","print(test[:5])\n","predict(train[:5] , test[:5], **myparam2)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["                                             comment  sentiment\n","0           里面座位并没有坐满 外面等候的人比较多 不知道为什么不让进去 要了一个面上的很慢          1\n","1                    非常满意的一次团购，菜品不错，自助餐里算不错的了，还是团购合适          1\n","2  服务越来越差 东西不如以前的好了 服务员一个个的真是素质太差了 这回进餐感觉非常不好 以后绝...          0\n","3  没办法了不是高峰期 点的菜得一样一样催 网络不好非要微信点餐 还得让顾客自己去前台拿电子菜单...          0\n","4  真的不知道为什么会这么火，排了那么半天，居然给我吃这个，点的几乎都是特色，感觉除了干酪鱼其他...          0\n","INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 5\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 5\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] 里 面 座 位 并 没 有 坐 满 外 面 等 候 的 人 比 较 多 不 知 道 为 什 么 不 让 进 去 要 了 一 个 面 上 的 很 慢 [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] 里 面 座 位 并 没 有 坐 满 外 面 等 候 的 人 比 较 多 不 知 道 为 什 么 不 让 进 去 要 了 一 个 面 上 的 很 慢 [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 7027 7481 2429 855 2400 3766 3300 1777 4007 1912 7481 5023 952 4638 782 3683 6772 1914 679 4761 6887 711 784 720 679 6375 6822 1343 6206 749 671 702 7481 677 4638 2523 2714 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 7027 7481 2429 855 2400 3766 3300 1777 4007 1912 7481 5023 952 4638 782 3683 6772 1914 679 4761 6887 711 784 720 679 6375 6822 1343 6206 749 671 702 7481 677 4638 2523 2714 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] 非 常 满 意 的 一 次 团 购 ， 菜 品 不 错 ， 自 助 餐 里 算 不 错 的 了 ， 还 是 团 购 合 适 [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] 非 常 满 意 的 一 次 团 购 ， 菜 品 不 错 ， 自 助 餐 里 算 不 错 的 了 ， 还 是 团 购 合 适 [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 7478 2382 4007 2692 4638 671 3613 1730 6579 8024 5831 1501 679 7231 8024 5632 1221 7623 7027 5050 679 7231 4638 749 8024 6820 3221 1730 6579 1394 6844 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 7478 2382 4007 2692 4638 671 3613 1730 6579 8024 5831 1501 679 7231 8024 5632 1221 7623 7027 5050 679 7231 4638 749 8024 6820 3221 1730 6579 1394 6844 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] 服 务 越 来 越 差 东 西 不 如 以 前 的 好 了 服 务 员 一 个 个 的 真 是 素 质 太 差 了 这 回 进 餐 感 觉 非 常 不 好 以 后 绝 不 会 再 来 了 [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] 服 务 越 来 越 差 东 西 不 如 以 前 的 好 了 服 务 员 一 个 个 的 真 是 素 质 太 差 了 这 回 进 餐 感 觉 非 常 不 好 以 后 绝 不 会 再 来 了 [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 3302 1218 6632 3341 6632 2345 691 6205 679 1963 809 1184 4638 1962 749 3302 1218 1447 671 702 702 4638 4696 3221 5162 6574 1922 2345 749 6821 1726 6822 7623 2697 6230 7478 2382 679 1962 809 1400 5318 679 833 1086 3341 749 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 3302 1218 6632 3341 6632 2345 691 6205 679 1963 809 1184 4638 1962 749 3302 1218 1447 671 702 702 4638 4696 3221 5162 6574 1922 2345 749 6821 1726 6822 7623 2697 6230 7478 2382 679 1962 809 1400 5318 679 833 1086 3341 749 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] 没 办 法 了 不 是 高 峰 期 点 的 菜 得 一 样 一 样 催 网 络 不 好 非 要 微 信 点 餐 还 得 让 顾 客 自 己 去 前 台 拿 电 子 菜 单 问 一 下 菜 品 是 什 么 做 的 一 个 服 务 员 不 理 人 另 一 个 人 说 刚 来 的 不 知 道 我 也 是 醉 了 白 堤 路 的 店 总 去 这 家 店 不 要 砸 招 牌 好 吗 [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] 没 办 法 了 不 是 高 峰 期 点 的 菜 得 一 样 一 样 催 网 络 不 好 非 要 微 信 点 餐 还 得 让 顾 客 自 己 去 前 台 拿 电 子 菜 单 问 一 下 菜 品 是 什 么 做 的 一 个 服 务 员 不 理 人 另 一 个 人 说 刚 来 的 不 知 道 我 也 是 醉 了 白 堤 路 的 店 总 去 这 家 店 不 要 砸 招 牌 好 吗 [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 3766 1215 3791 749 679 3221 7770 2292 3309 4157 4638 5831 2533 671 3416 671 3416 998 5381 5317 679 1962 7478 6206 2544 928 4157 7623 6820 2533 6375 7560 2145 5632 2346 1343 1184 1378 2897 4510 2094 5831 1296 7309 671 678 5831 1501 3221 784 720 976 4638 671 702 3302 1218 1447 679 4415 782 1369 671 702 782 6432 1157 3341 4638 679 4761 6887 2769 738 3221 7004 749 4635 1837 6662 4638 2421 2600 1343 6821 2157 2421 679 6206 4790 2875 4277 1962 1408 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 3766 1215 3791 749 679 3221 7770 2292 3309 4157 4638 5831 2533 671 3416 671 3416 998 5381 5317 679 1962 7478 6206 2544 928 4157 7623 6820 2533 6375 7560 2145 5632 2346 1343 1184 1378 2897 4510 2094 5831 1296 7309 671 678 5831 1501 3221 784 720 976 4638 671 702 3302 1218 1447 679 4415 782 1369 671 702 782 6432 1157 3341 4638 679 4761 6887 2769 738 3221 7004 749 4635 1837 6662 4638 2421 2600 1343 6821 2157 2421 679 6206 4790 2875 4277 1962 1408 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] 真 的 不 知 道 为 什 么 会 这 么 火 ， 排 了 那 么 半 天 ， 居 然 给 我 吃 这 个 ， 点 的 几 乎 都 是 特 色 ， 感 觉 除 了 干 酪 鱼 其 他 全 是 雷 ， 炒 肝 的 味 道 还 算 过 得 去 ， 那 个 牛 肉 烩 饭 ？ ！ 什 么 鬼 没 有 牛 肉 只 有 不 咸 不 淡 的 牛 肉 汤 泡 饭 ， 还 有 那 个 海 鲜 疙 瘩 汤 ， 海 鲜 在 哪 ？ 别 人 评 价 里 说 有 鱿 鱼 丁 ， 我 发 誓 ！ 一 个 丁 都 没 得 ！ [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] 真 的 不 知 道 为 什 么 会 这 么 火 ， 排 了 那 么 半 天 ， 居 然 给 我 吃 这 个 ， 点 的 几 乎 都 是 特 色 ， 感 觉 除 了 干 酪 鱼 其 他 全 是 雷 ， 炒 肝 的 味 道 还 算 过 得 去 ， 那 个 牛 肉 烩 饭 ？ ！ 什 么 鬼 没 有 牛 肉 只 有 不 咸 不 淡 的 牛 肉 汤 泡 饭 ， 还 有 那 个 海 鲜 疙 瘩 汤 ， 海 鲜 在 哪 ？ 别 人 评 价 里 说 有 鱿 鱼 丁 ， 我 发 誓 ！ 一 个 丁 都 没 得 ！ [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 4696 4638 679 4761 6887 711 784 720 833 6821 720 4125 8024 2961 749 6929 720 1288 1921 8024 2233 4197 5314 2769 1391 6821 702 8024 4157 4638 1126 725 6963 3221 4294 5682 8024 2697 6230 7370 749 2397 6991 7824 1071 800 1059 3221 7440 8024 4143 5498 4638 1456 6887 6820 5050 6814 2533 1343 8024 6929 702 4281 5489 4175 7649 8043 8013 784 720 7787 3766 3300 4281 5489 1372 3300 679 1496 679 3909 4638 4281 5489 3739 3796 7649 8024 6820 3300 6929 702 3862 7831 4546 4609 3739 8024 3862 7831 1762 1525 8043 1166 782 6397 817 7027 6432 3300 7825 7824 672 8024 2769 1355 6292 8013 671 702 672 6963 3766 2533 8013 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 4696 4638 679 4761 6887 711 784 720 833 6821 720 4125 8024 2961 749 6929 720 1288 1921 8024 2233 4197 5314 2769 1391 6821 702 8024 4157 4638 1126 725 6963 3221 4294 5682 8024 2697 6230 7370 749 2397 6991 7824 1071 800 1059 3221 7440 8024 4143 5498 4638 1456 6887 6820 5050 6814 2533 1343 8024 6929 702 4281 5489 4175 7649 8043 8013 784 720 7787 3766 3300 4281 5489 1372 3300 679 1496 679 3909 4638 4281 5489 3739 3796 7649 8024 6820 3300 6929 702 3862 7831 4546 4609 3739 8024 3862 7831 1762 1525 8043 1166 782 6397 817 7027 6432 3300 7825 7824 672 8024 2769 1355 6292 8013 671 702 672 6963 3766 2533 8013 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["Predictions on test file\n","INFO:tensorflow:Calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from output/model.ckpt-150\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from output/model.ckpt-150\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["{'probabilities': array([-0.00606802, -5.1077547 ], dtype=float32), 'labels': 0}\n","{'probabilities': array([-4.0730515 , -0.01717199], dtype=float32), 'labels': 1}\n","{'probabilities': array([-3.5364013e-03, -5.6464138e+00], dtype=float32), 'labels': 0}\n","{'probabilities': array([-1.8872085e-03, -6.2736158e+00], dtype=float32), 'labels': 0}\n","{'probabilities': array([-2.4587659e-03, -6.0093441e+00], dtype=float32), 'labels': 0}\n"],"name":"stdout"}]}]}