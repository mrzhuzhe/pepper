{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Connect 4 - Deep Reinforcement Learning introduction",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrzhuzhe/pepper/blob/master/RL/Connect_4_Deep_Reinforcement_Learning_introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16Gu6oVnT116"
      },
      "source": [
        "# Learning to play connect 4 using minimax Deep Q-learning\n",
        "In this notebook we will train a reinforcement learning (RL) agent using minimax deep Q-learning on a classic game: Connect 4. \n",
        "\n",
        "In Connect 4, your objective is to get 4 of your checkers in a row horizontally, vertically, or diagonally on the game board before your opponent. When it's your turn, you “drop” one of your checkers into one of the columns at the top of the board. Then, let your opponent take their turn. This means each move may be trying to either win for you, or trying to stop your opponent from winning.\n",
        "\n",
        "See the [Kaggle competition](https://www.kaggle.com/c/connectx) for more background and the [thread](https://www.kaggle.com/c/connectx/discussion/129145) that discusses this notebook. This [high level presentation](https://docs.google.com/presentation/d/1bNwOMZq1_poMRm6zPEFtEuCTSYd5u8OD9HbM4X6PsuI/edit?usp=sharing) on using RL in board games may also be useful. I adapted code from some of the public notebooks but developed all of the RL logic myself.\n",
        "\n",
        "![alt text](https://storage.googleapis.com/kaggle-media/competitions/ConnectX/Walter's%20image.png)\n",
        "\n",
        "Last modified on Feb 10th 2020 (Tom Van de Wiele)\n",
        "\n",
        "# 成功的snapshot\n",
        "https://colab.research.google.com/drive/1igIrGSmr30FuWfJ-nMmCmIwgthT6AvqI#scrollTo=ApMTvY6bpdmT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_25XvOMgUJ42"
      },
      "source": [
        "## Install dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSUwDiiwUMo4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd9a974f-232c-427e-ac14-5b2fc89e3f4b"
      },
      "source": [
        "!pip install 'kaggle-environments>=0.1.6'\n",
        "!pip install 'recordtype'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kaggle-environments>=0.1.6\n",
            "  Downloading kaggle_environments-1.8.12-py2.py3-none-any.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 9.1 MB/s \n",
            "\u001b[?25hCollecting jsonschema>=3.0.1\n",
            "  Downloading jsonschema-4.2.0-py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->kaggle-environments>=0.1.6) (0.18.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->kaggle-environments>=0.1.6) (5.2.2)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->kaggle-environments>=0.1.6) (21.2.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->kaggle-environments>=0.1.6) (4.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->jsonschema>=3.0.1->kaggle-environments>=0.1.6) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->jsonschema>=3.0.1->kaggle-environments>=0.1.6) (3.7.4.3)\n",
            "Installing collected packages: jsonschema, kaggle-environments\n",
            "  Attempting uninstall: jsonschema\n",
            "    Found existing installation: jsonschema 2.6.0\n",
            "    Uninstalling jsonschema-2.6.0:\n",
            "      Successfully uninstalled jsonschema-2.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "nbclient 0.5.4 requires jupyter-client>=6.1.5, but you have jupyter-client 5.3.5 which is incompatible.\u001b[0m\n",
            "Successfully installed jsonschema-4.2.0 kaggle-environments-1.8.12\n",
            "Collecting recordtype\n",
            "  Downloading recordtype-1.3-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from recordtype) (1.15.0)\n",
            "Installing collected packages: recordtype\n",
            "Successfully installed recordtype-1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpqBQiAaYEqN"
      },
      "source": [
        "## Google Colab keyboard shortcuts\n",
        "\n",
        "*   Create a new cell : (CTRL + m) + a\n",
        "*   Delete a new cell : (CTRL + m) + d\n",
        "*   Execute cell and move on to the next one: CTRL + ENTER\n",
        "*   Toggle the contents of cells under a title: CTRL + #\n",
        "\n",
        "## Some Colab tips\n",
        "Double click on the white input fields to toggle between hiding and showing the code.  Try it out on this cell, it also works for text cells!\n",
        "\n",
        "You can use the arrows on the left to toggle between hiding and showing the code for one/multiple cells (equivalent to CTRL + #).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x44fD2xNVcCG"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlXzE0GdVdyD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cad60ec5-e701-430a-effb-684348c08290"
      },
      "source": [
        "#@title Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "%tensorflow_version 1.x # See https://colab.research.google.com/notebooks/tensorflow_version.ipynb\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from IPython.display import display\n",
        "from IPython.display import Image\n",
        "from kaggle_environments import evaluate as evaluate_game\n",
        "from kaggle_environments import make as make_game\n",
        "from kaggle_environments import utils as utils_game\n",
        "from random import choice\n",
        "from recordtype import recordtype\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.x # See https://colab.research.google.com/notebooks/tensorflow_version.ipynb`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n",
            "Loading environment football failed: No module named 'gfootball'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uQg_9a7XJHI"
      },
      "source": [
        "## Utility functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7jbw3c3XNbd"
      },
      "source": [
        "#@title Various utilities\n",
        "\n",
        "ExperienceStep = recordtype('ExperienceStep', [\n",
        "  'game_id',\n",
        "  'current_network_input',\n",
        "  'action',\n",
        "  'next_network_input',\n",
        "  'last_episode_action',\n",
        "  'episode_reward',\n",
        "  ])\n",
        "\n",
        "# Collect user input - Modified from https://www.kaggle.com/marcovasquez/how-to-play-with-computer-and-check-winner\n",
        "def get_input(user, observation, configuration):\n",
        "  ncol = configuration.columns\n",
        "  time.sleep(0.1)\n",
        "  input1 = 'Input from player {}: '.format(your_name)\n",
        "  while True:\n",
        "    try:\n",
        "      print('Enter Value from 1 to 7')\n",
        "      raw_input = input(input1)\n",
        "      user_input = int(raw_input)\n",
        "    except ValueError:\n",
        "      try:\n",
        "        print('Invalid input:', user_input)\n",
        "        continue\n",
        "      except UnboundLocalError:\n",
        "        user_input = -1\n",
        "        if raw_input == 'q':\n",
        "          break\n",
        "        continue\n",
        "    np_board = obs_to_board(observation, configuration)\n",
        "    valid_actions = np.where(np_board[0] == 0)[0]\n",
        "    if user_input <= 0 or user_input > ncol or (\n",
        "        user_input-1) not in valid_actions:\n",
        "      print('invalid input:', user_input)\n",
        "      print('Valid actions: {}'.format(valid_actions+1))\n",
        "    else:\n",
        "      return user_input-1\n",
        "\n",
        "\n",
        "# Convert the 1D observation list to a 2D numpy array\n",
        "def obs_to_board(observation, configuration):\n",
        "  return np.array(observation.board).reshape(\n",
        "    configuration.rows, configuration.columns)\n",
        "  \n",
        "\n",
        "def check_winner(observation):\n",
        "  '''\n",
        "  Source: https://www.kaggle.com/marcovasquez/how-to-play-with-computer-and-check-winner\n",
        "  This function returns the value of the winner.\n",
        "  \n",
        "  INPUT:  observation \n",
        "  OUTPUT: 1 for user Winner or 2 for Computer Winner \n",
        "  '''\n",
        "  line1 = observation.board[0:7] # bottom row\n",
        "  line2 = observation.board[7:14]\n",
        "  line3 = observation.board[14:21]\n",
        "  line4 = observation.board[21:28]\n",
        "  line5 = observation.board[28:35]\n",
        "  line6 = observation.board[35:42]\n",
        "\n",
        "  board = [line1, line2 , line3, line4, line5, line6] \n",
        "\n",
        "  # Check rows for winner\n",
        "  for row in range(6):\n",
        "    for col in range(4):\n",
        "      if (board[row][col] == board[row][col + 1] == board[row][col + 2] == (\n",
        "        board[row][col + 3])) and (board[row][col] != 0):\n",
        "        return board[row][col]  #Return Number that match row\n",
        "\n",
        "  # Check columns for winner\n",
        "  for col in range(7):\n",
        "    for row in range(3):\n",
        "      if (board[row][col] == board[row + 1][col] == board[row + 2][col] == (\n",
        "        board[row + 3][col])) and (board[row][col] != 0):\n",
        "        return board[row][col]  #Return Number that match column\n",
        "\n",
        "  # Check diagonal (top-left to bottom-right) for winner\n",
        "  for row in range(3):\n",
        "    for col in range(4):\n",
        "      if (board[row][col] == board[row + 1][col + 1] == board[\n",
        "          row + 2][col + 2] ==\\\n",
        "        board[row + 3][col + 3]) and (board[row][col] != 0):\n",
        "        return board[row][col] #Return Number that match diagonal\n",
        "\n",
        "\n",
        "  # Check diagonal (bottom-left to top-right) for winner\n",
        "  for row in range(5, 2, -1):\n",
        "    for col in range(4):\n",
        "      if (board[row][col] == board[row - 1][col + 1] == (\n",
        "          board[row - 2][col + 2]) == board[row - 3][col + 3]) and (\n",
        "            board[row][col] != 0):\n",
        "        return board[row][col] #Return Number that match diagonal\n",
        "\n",
        "  # No winner: return None\n",
        "  return None\n",
        "\n",
        "\n",
        "# Custom class to reuse data of subsequent interations with the environment\n",
        "# FIFO buffer. Experience buffer (also referred to as the replay buffer).\n",
        "class ExperienceBuffer:\n",
        "  def __init__(self, buffer_size):\n",
        "    self.buffer_size = buffer_size\n",
        "    self.episode_offset = 0\n",
        "    self.data = []\n",
        "    self.episode_ids = np.array([])\n",
        "    \n",
        "  def add(self, data):\n",
        "    episode_ids = np.array([d.game_id for d in data])\n",
        "    num_episodes = episode_ids[-1] + 1\n",
        "    if num_episodes > self.buffer_size:\n",
        "      # Keep most recent experience of the experience batch\n",
        "      data = data[\n",
        "        np.where(episode_ids == (num_episodes-self.buffer_size))[0][0]:]\n",
        "      self.data = data\n",
        "      self.episode_ids = episode_ids\n",
        "      self.episode_offset = 0\n",
        "      return\n",
        "      \n",
        "    episode_ids = episode_ids + self.episode_offset\n",
        "    self.data = data + self.data\n",
        "    self.episode_ids = np.concatenate([episode_ids, self.episode_ids])\n",
        "    \n",
        "    unique_episode_ids = pd.unique(self.episode_ids)\n",
        "    if unique_episode_ids.size > self.buffer_size:\n",
        "      cutoff_index = np.where(self.episode_ids == unique_episode_ids[\n",
        "        self.buffer_size])[0][0]\n",
        "      self.data = self.data[:cutoff_index]\n",
        "      self.episode_ids = self.episode_ids[:cutoff_index]\n",
        "    self.episode_offset += num_episodes\n",
        "    \n",
        "  def get_all_data(self):\n",
        "    return self.data\n",
        "\n",
        "  def size(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def num_episodes(self):\n",
        "    return np.unique(self.episode_ids).size"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar75F4sLVx6o"
      },
      "source": [
        "## Let's play against a random agent to try out the game - q to exit the game"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A8RExo3Vrdc"
      },
      "source": [
        "your_name = 'Tom' #@param {type:\"string\"}\n",
        "play_against_random = False #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "plot_resolution = 400 #@param {type:\"slider\", min:200, max:500, step:1}\n",
        "# Here we define an agent that picks a random non-empty column\n",
        "def my_random_agent(observation, configuration):\n",
        "  return int(choice([c for c in range(\n",
        "      configuration.columns) if observation.board[c] == 0]))\n",
        "    \n",
        "def play_against_agent(opponent_agent):\n",
        "  # Play as first position against the opposing agent.\n",
        "  # modified from https://www.kaggle.com/marcovasquez/how-to-play-with-computer-and-check-winner\n",
        "  env = make_game(\"connectx\", debug=False, configuration={\"timeout\": 10})\n",
        "  trainer = env.train([None, opponent_agent])\n",
        "  observation = trainer.reset()\n",
        "\n",
        "  while not env.done:\n",
        "    clear_output(wait=True) # Comment if you want to keep track of every action\n",
        "    print(\"{}'s color: Blue\".format(your_name))\n",
        "    env.render(mode=\"ipython\", width=plot_resolution, height=plot_resolution,\n",
        "              header=False, controls=False)\n",
        "\n",
        "    my_action = get_input(your_name, observation, env.configuration)\n",
        "    if my_action is None:\n",
        "      print(\"Exiting game after pressing q\")\n",
        "      return\n",
        "\n",
        "    observation, reward, done, info = trainer.step(my_action)\n",
        "    #print(observation, reward, done, info)\n",
        "    if (check_winner(observation) == 1):\n",
        "      print(\"You Won, Amazing! \\nGAME OVER\")\n",
        "        \n",
        "    elif (check_winner(observation) == 2):\n",
        "      print(\"The opponent Won! \\nGAME OVER\")\n",
        "\n",
        "  if (check_winner(observation) is None):\n",
        "    print(\"That is a draw between you and the opponent\")\n",
        "\n",
        "  env.render(mode=\"ipython\", width=plot_resolution, height=plot_resolution,\n",
        "            header=False, controls=False)\n",
        "  \n",
        "if play_against_random:\n",
        "  play_against_agent(my_random_agent)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcC0tZc_jKS8"
      },
      "source": [
        "## That was too easy, we should obviously train a deep reinforcement learning agent to play against.\n",
        "\n",
        "In this tutorial we will be using Q-learning to train an agent. [How does Q-learning work?](https://https://en.wikipedia.org/wiki/Q-learning) The Q-values represent the expected terminal reward when taking action **a** in board state **s**. A win is associated with a terminal reward of 1, a loss with a 0 reward and a draw with a 0.5 reward. The learning process looks as follows:\n",
        "\n",
        "Randomly initialize Q(s) - a mapping from the current board state to the Q-values of all actions for the current state. \n",
        "\n",
        "While True:\n",
        "\n",
        "    1.   Play a fixed number of games using self-play and add the experience to the experience buffer\n",
        "    2.   Update the Q network using the experience from the experience buffer\n",
        "    3.   Evaluate the trained agent against a random agent to find out if we are making progress \n",
        "\n",
        "  \n",
        "Attention! This notebook intentionally introduced 2 obvious bugs and one configuration setting that is obviously suboptimal. The bugs and their fixes are listed at the bottom of the notebook but please try to find and fix them yourself first!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhYVuLrBXVhk"
      },
      "source": [
        "#@title Define possible network architectures of the Q-network\n",
        "# Basic MLP Connect 4 network\n",
        "def mlp_connect4(config):\n",
        "  inputs = Input((6, 7, 3), name='encoded_board')\n",
        "  x = inputs\n",
        "  \n",
        "  # Flatten the input\n",
        "  x = Flatten()(x)\n",
        "  \n",
        "  # MLP layers - sigmoid activation on the final layer\n",
        "  for i, layer_size in enumerate(config['mlp_layers']):\n",
        "    x = Dense(layer_size, activation='linear')(x)\n",
        "    if i < (len(config['mlp_layers'])-1):\n",
        "      x = Activation('relu')(x)\n",
        "    else:\n",
        "      x = Activation('sigmoid', name='Q-values')(x)\n",
        "    \n",
        "  outputs = x\n",
        "  \n",
        "  return (inputs, outputs)\n",
        "\n",
        "# Basic convolutional Connect 4 network\n",
        "def convnet_connect4(config):\n",
        "  inputs = Input((6, 7, 3), name='encoded_board')\n",
        "  x = inputs\n",
        "  \n",
        "  # Convolutional layers\n",
        "  conv_outputs = []\n",
        "  for i, (filters, kernel, strides) in enumerate(\n",
        "      config['filters_kernels_strides']):\n",
        "    x = Conv2D(filters=filters, kernel_size=kernel, strides=strides,\n",
        "               padding='same', activation='linear')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    \n",
        "  # Flatten the activations\n",
        "  x = Flatten()(x)\n",
        "  \n",
        "  # MLP layers - sigmoid activation on the final layer\n",
        "  for i, layer_size in enumerate(config['mlp_layers']):   \n",
        "    x = Dense(layer_size, activation='linear')(x)   \n",
        "    if i < (len(config['mlp_layers'])-1):\n",
        "      x = Activation('relu')(x)\n",
        "    else:\n",
        "      # Head of the network\n",
        "      outputs = Activation('sigmoid', name='Q-values')(x)\n",
        "  \n",
        "  return (inputs, outputs)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A81hWsB9tTZB"
      },
      "source": [
        "#@title Network evaluation and action selection\n",
        "\n",
        "# Convert the raw observation to the network input - this is the place to add\n",
        "# engineered features\n",
        "def obs_to_network_input(observation, configuration, player_id):\n",
        "  board = np.array(observation['board']).reshape(\n",
        "    configuration.rows, configuration.columns)\n",
        "  \n",
        "  # One hot encoding of the inputs (empty, player 1, player 2)\n",
        "  obs_input = (np.arange(3) == board[..., None]).astype(float)\n",
        "  \n",
        "  # Swap player 1 and player 2 positions? The network always assumes that 'my'\n",
        "  # stones (player_id) come first\n",
        "  if player_id == 1:\n",
        "    tmp = obs_input[:, :, 1].copy()\n",
        "    obs_input[:, :, 1] = obs_input[:, :, 2]\n",
        "    obs_input[:, :, 2] = tmp\n",
        "  \n",
        "  return obs_input\n",
        "\n",
        "\n",
        "# Predict network outputs where the number of inputs can be large. Use batching\n",
        "# when there are more inputs than the max_batch_size\n",
        "def my_keras_predict(model, inputs, max_batch_size=10000):\n",
        "  num_inputs = inputs.shape[0]\n",
        "  num_batches = int(np.ceil(num_inputs/max_batch_size))\n",
        "  \n",
        "  outputs = []\n",
        "  for i in range(num_batches):\n",
        "    end_id = num_inputs if i == (num_batches-1) else (i+1)*max_batch_size\n",
        "    batch_inputs = inputs[i*max_batch_size:end_id]\n",
        "    if tf.__version__[0] == '2':\n",
        "      batch_outputs = model(batch_inputs)\n",
        "      batch_outputs = batch_outputs.numpy()\n",
        "    else:\n",
        "      batch_outputs = model.predict(batch_inputs)\n",
        "    outputs.append(batch_outputs)\n",
        "\n",
        "  return np.concatenate([o for o in outputs])\n",
        "\n",
        "def select_action_from_q(q_values, valid_actions, epsilon_greedy_parameter): \n",
        "  # Select the best valid or a valid exploratory action using epsilon-greedy\n",
        "  best_q = q_values[valid_actions].max()\n",
        "  best_a_ids = np.where(q_values[valid_actions] == best_q)[0]\n",
        "  best_a = valid_actions[np.random.choice(best_a_ids)]\n",
        "  exploratory_a = np.random.choice(valid_actions)\n",
        "  explore = np.random.uniform() < epsilon_greedy_parameter\n",
        "  action = exploratory_a if explore else best_a\n",
        "    \n",
        "  return action\n",
        "\n",
        "\n",
        "def get_agent_q_and_a(agent, board, epsilon_greedy_parameter):  \n",
        "  # Obtain the Q-values\n",
        "  q_values = my_keras_predict(agent, np.expand_dims(board, 0))[0]\n",
        "  \n",
        "  # Select an action from the Q-values\n",
        "  valid_actions = np.where(board[0, :, 0] == 1)[0]\n",
        "  action = select_action_from_q(q_values, valid_actions,\n",
        "                                epsilon_greedy_parameter)\n",
        "  \n",
        "  return q_values, action"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApMTvY6bpdmT"
      },
      "source": [
        "#@title Self play\n",
        "def self_play(agent, num_games, verbose, epsilon_greedy_parameter):\n",
        "  experience = []\n",
        "  for game_id in range(num_games):\n",
        "    this_game_data = []\n",
        "    env = make_game('connectx')\n",
        "    env.reset()\n",
        "    episode_step = 0\n",
        "    \n",
        "    # Take actions until the game is terminated\n",
        "    while not env.done:\n",
        "      if env.state[0].status == 'ACTIVE':\n",
        "        player_id = 0\n",
        "      elif env.state[1].status == 'ACTIVE':\n",
        "        player_id = 1\n",
        "      #print(env.state[player_id].observation)\n",
        "\n",
        "      #if not hasattr(env.state[player_id].observation, 'board'):\n",
        "      #  continue\n",
        "      #print(env.state)\n",
        "      # Obtain the Q-values and selected action for the current state\n",
        "      current_network_input = obs_to_network_input(\n",
        "          env.state[0].observation, env.configuration, player_id)\n",
        "      q_values, action = get_agent_q_and_a(\n",
        "        agent, current_network_input, epsilon_greedy_parameter)\n",
        "      \n",
        "      if episode_step == 0 and game_id == 0:\n",
        "        print(\"Start move Q-values: {}\".format(np.around(q_values, 3)))\n",
        "\n",
        "      env.step([int(action) if i == player_id else None for i in [0, 1]])\n",
        "\n",
        "      # Store the state transition data - swap the player id!\n",
        "      next_network_input = obs_to_network_input(\n",
        "          env.state[0].observation, env.configuration, 1-player_id)\n",
        "      this_game_data.append(ExperienceStep(\n",
        "        game_id,\n",
        "        current_network_input,\n",
        "        action,\n",
        "        next_network_input,\n",
        "        False, # Last episode action, overwritten at the end of the episode\n",
        "        np.nan, # Terminal reward, overwritten at the end of the episode\n",
        "        ))\n",
        "      episode_step += 1\n",
        "      \n",
        "    # Overwrite the terminal reward for all actions\n",
        "    first_terminal_reward = env.state[0].reward\n",
        "    for i in range(len(this_game_data)):\n",
        "      if i % 2 == 0:\n",
        "        this_game_data[i].episode_reward = first_terminal_reward\n",
        "      else:\n",
        "        this_game_data[i].episode_reward = 1-first_terminal_reward\n",
        "    \n",
        "    # Update statistics which can not be computed before the episode is over.\n",
        "    this_game_data[-1].last_episode_action = True # Last episode action\n",
        "    \n",
        "    experience.extend(this_game_data)\n",
        "\n",
        "    if verbose and game_id % 10 == 9:\n",
        "      print('Completed playing game {} of {}'.format(game_id+1, num_games))\n",
        "\n",
        "  return experience"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWdlFK9-CDSy"
      },
      "source": [
        "#@title One step minimax Q-learning target computation - Source of name: https://arxiv.org/pdf/1901.00137.pdf. To be more precise, we actually use negamax Q-learning since we rely on the property that the game is a two-player zero sum game.\n",
        "def one_step_minimax_q_targets(next_q_vals, experience):\n",
        "  next_q_minimax_star = (1-next_q_vals.max(1)).tolist() # Negamax - 2 player 0-sum\n",
        "  terminal_rewards = [e.episode_reward for e in experience]\n",
        "  last_episode_actions = [e.last_episode_action for e in experience]\n",
        "  #target_qs = next_q_minimax_star\n",
        "  target_qs = np.array([t if l else n for(t, l, n) in zip(\n",
        "    terminal_rewards, last_episode_actions, next_q_minimax_star)])\n",
        "  \n",
        "  return target_qs\n",
        "\n",
        "\n",
        "# N-step minimax Q-learning target computation\n",
        "def minimax_q_n_step_targets(this_q_vals, next_q_vals, experience,\n",
        "                          return_steps_trace):\n",
        "  # Collect generic experience values of interest\n",
        "  return_steps, lambda_ = return_steps_trace\n",
        "  actions = [e.action for e in experience]\n",
        "  terminal_actions = np.concatenate(\n",
        "    [np.array([e.last_episode_action for e in experience]),\n",
        "     np.zeros((return_steps), dtype=np.bool)])\n",
        "  terminal_rewards = 1-np.array([e.episode_reward for e in experience])\n",
        "  num_experience_steps = len(experience)\n",
        "  \n",
        "  # Determine if the actions are exploratory or greedy\n",
        "  greedy_actions = np.zeros((num_experience_steps+return_steps),\n",
        "                            dtype=np.bool)\n",
        "  for i in range(num_experience_steps):\n",
        "    valid_actions = np.where(experience[i].current_network_input[:, :, 0].sum(\n",
        "        0) > 0)[0]\n",
        "    best_valid_q = this_q_vals[i][valid_actions].max()\n",
        "    greedy_actions[i] = best_valid_q == this_q_vals[i, actions[i]]\n",
        "  \n",
        "  # Extend and overwrite next q vals to include the true rewards\n",
        "  next_q_vals = np.concatenate([next_q_vals, -999*np.ones((\n",
        "    return_steps, next_q_vals.shape[1]))])\n",
        "  next_q_vals[terminal_actions] = np.tile(\n",
        "    np.expand_dims(terminal_rewards[terminal_actions[\n",
        "      :num_experience_steps]], -1), [1, next_q_vals.shape[1]])\n",
        "  \n",
        "  # Consider returns, up to 'return_steps' into the future. The trace is cut\n",
        "  # at episode boundaries and before considering a non-exploratory action\n",
        "  consider_targets = np.ones((num_experience_steps), dtype=np.bool)\n",
        "  target_lambda_sums = np.zeros((num_experience_steps))\n",
        "  target_weighted_sums = np.zeros((num_experience_steps))\n",
        "  trace_multiplier = 1\n",
        "  for i in range(return_steps):\n",
        "    best_qs = next_q_vals[i:(i+num_experience_steps)].max(-1)\n",
        "    if i % 2 == 0:\n",
        "      best_qs = 1-best_qs\n",
        "    target_weighted_sums[consider_targets] += trace_multiplier*best_qs[\n",
        "      consider_targets]\n",
        "    target_lambda_sums[consider_targets] += trace_multiplier\n",
        "      \n",
        "    # Don't consider any further targets if this was the episode terminal\n",
        "    # action\n",
        "    consider_targets = np.logical_and(\n",
        "        consider_targets, np.logical_not(\n",
        "          terminal_actions[i:(i+num_experience_steps)]))\n",
        "    \n",
        "    # Don't consider the target if the next action is exploratory\n",
        "    # Extending greedy actions with return_steps False values makes sure\n",
        "    # we don't consider N step returns where there is no data\n",
        "    consider_targets = np.logical_and(\n",
        "      consider_targets, greedy_actions[(i+1):(i+1+num_experience_steps)])\n",
        "    trace_multiplier *= lambda_\n",
        "    \n",
        "  targets = target_weighted_sums/target_lambda_sums\n",
        "  \n",
        "  return targets\n",
        "\n",
        "\n",
        "# Get the q-learning observations and targets\n",
        "def minimax_q_learning(model, experience, nan_coding_value, return_steps_trace):\n",
        "  # Evaluate the Q-values of the current and next state for all observations\n",
        "  num_actions = experience[0].current_network_input.shape[1]\n",
        "  num_steps = len(experience)\n",
        "  this_states = np.stack([e.current_network_input for e in experience])\n",
        "  next_states = np.stack([e.next_network_input for e in experience])\n",
        "  this_q_vals = my_keras_predict(model, this_states)\n",
        "  next_q_vals = my_keras_predict(model, next_states)\n",
        "    \n",
        "  # Filter out next Q-values where the next action is not valid. Filtered out\n",
        "  # since every target computation performs a max operation.\n",
        "  # This was an unintended bug in the original version of the notebook!\n",
        "  next_q_vals[next_states[:, 0, :, 0] == 0] = -1\n",
        "\n",
        "  # Compute the target Q-values\n",
        "  num_return_steps = return_steps_trace[0]\n",
        "  if num_return_steps == 1:\n",
        "    target_qs = one_step_minimax_q_targets(next_q_vals, experience)\n",
        "  else:\n",
        "    target_qs = minimax_q_n_step_targets(this_q_vals, next_q_vals, experience,\n",
        "                                      return_steps_trace)\n",
        "  \n",
        "  # Don't learn about non acted Q-values\n",
        "  all_target_qs = nan_coding_value*np.ones([num_steps, num_actions])\n",
        "    \n",
        "  # Set the targets for the actions that were selected\n",
        "  actions = [e.action for e in experience]\n",
        "  all_target_qs[np.arange(num_steps), actions] = target_qs\n",
        "    \n",
        "  return this_states, all_target_qs\n",
        "\n",
        "\n",
        "# Masked mse loss - values equal to mask_val are ignored in the loss\n",
        "def masked_mse(y, p, mask_val):\n",
        "  mask = K.cast(K.not_equal(y, mask_val), K.floatx())\n",
        "  if tf.__version__[0] == '2':\n",
        "    masked_loss = tf.losses.mse(y*mask, p*mask)\n",
        "  else:\n",
        "    mask = K.cast(mask, 'float32')\n",
        "    masked_loss = K.mean(tf.math.square(p*mask - y*mask), axis=-1)\n",
        "    # masked_loss = tf.compat.v1.losses.mean_squared_error(y*mask, p*mask)\n",
        "    \n",
        "  return masked_loss\n",
        "    \n",
        "# Make the masked mse loss\n",
        "def make_masked_mse(nan_coding_value):\n",
        "  def loss(y, p):\n",
        "    return masked_mse(y, p, mask_val=nan_coding_value)\n",
        "  \n",
        "  return loss\n",
        "\n",
        "\n",
        "# Update the Q-network by minimzing the difference with the target q-values\n",
        "def update_agent(experience, agent, config):\n",
        "    nan_coding_value = config['nan_coding_value']\n",
        "    return_steps_trace = config['return_steps_trace']\n",
        "    x_train, y_train = minimax_q_learning(\n",
        "      agent, experience, nan_coding_value, return_steps_trace)\n",
        "    adam = Adam(lr=config['learning_rate'])\n",
        "    agent.compile(optimizer=adam, loss=make_masked_mse(nan_coding_value))\n",
        "    \n",
        "    agent.fit(\n",
        "      x_train,\n",
        "      y_train,\n",
        "      batch_size=config['batch_size'], \n",
        "      epochs=config['num_epochs'],\n",
        "      verbose=config['verbose_fit']\n",
        "      )"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYwiTQuPLUXA"
      },
      "source": [
        "#@title Evaluate a greedy agent against the random agent\n",
        "def eval_greedy_versus_random(agent, num_eval_games):\n",
        "  rewards = []\n",
        "  env = make_game('connectx', debug=False)\n",
        "  my_agent_starts = False\n",
        "  for i in range(num_eval_games):\n",
        "    my_agent_starts = not my_agent_starts # Alternate start turns\n",
        "    player_id = int(not my_agent_starts)\n",
        "\n",
        "    episode_step = 0\n",
        "    while not env.done:\n",
        "      if env.state[0].status == 'ACTIVE':\n",
        "        active = 0\n",
        "      elif env.state[1].status == 'ACTIVE':\n",
        "        active = 1\n",
        "\n",
        "      current_network_input = obs_to_network_input(\n",
        "          env.state[0].observation, env.configuration, player_id)\n",
        "      if active == player_id:\n",
        "        # Take the greedy action of the agent\n",
        "        _, action = get_agent_q_and_a(\n",
        "            agent, current_network_input, epsilon_greedy_parameter=0)\n",
        "      else:\n",
        "        # Take a random valid action\n",
        "        valid_actions = np.where(current_network_input[0, :, 0] == 1)[0]\n",
        "        action = np.random.choice(valid_actions)\n",
        "      \n",
        "      env.step([int(action) if i == active else None for i in [0, 1]])\n",
        "      episode_step += 1\n",
        "\n",
        "    rewards.append(env.state[player_id].reward)\n",
        "    env.reset()\n",
        "\n",
        "  return np.array(rewards).mean()"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Npr-SURTtsmj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3be17067-a840-404b-9c95-5d2c562e1aa6"
      },
      "source": [
        "#@title Main logic - interrupt manually since this will run forever. Without bugs, the agents should consistently beat the random agent > 90% of the time after <2000 games and eventually win all games with slightly better configuration settings. A non-buggy agent usually prefers central opening moves (higher Q-value). The buggy agent has no action preference at any state (hint to find one of the bugs!). Second hint: does the printed number of transitions look weird to you? It should!\n",
        "# The network weights are saved in this session which is bound to be brittle - store to and load from file when expanding on this logic!\n",
        "reset_agent_weights = False #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "reset_experience_buffer = False #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "plot_agent_architecture = True #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "config = {\n",
        "    'model': [mlp_connect4, convnet_connect4][0],\n",
        "    'model_config': {\n",
        "        'filters_kernels_strides': [\n",
        "      (32, 3, 1), (32, 3, 2), (32, 3, 2)],\n",
        "      'mlp_layers': [64, 7],\n",
        "    },\n",
        "    'num_self_play_games_per_iteration': 200,\n",
        "    'verbose_self_play': False, # Show self-play game id progress?\n",
        "    'epsilon_greedy_parameter': 0.0,\n",
        "\n",
        "    'max_experience_buffer_games': 2000,\n",
        "\n",
        "    'num_learning_updates_per_iteration': 2,\n",
        "    'learning_config': {\n",
        "        # Return-steps: N in N-step returns and lambda (only for N > 1)\n",
        "        'return_steps_trace': (1, 1), \n",
        "        'nan_coding_value': -999,\n",
        "        'learning_rate': 5e-4,\n",
        "        'batch_size': 32,\n",
        "        'num_epochs': 2,\n",
        "        'verbose_fit': False, # Show learning progress?\n",
        "    },\n",
        "    \n",
        "    'num_evaluation_games': 50,\n",
        "}\n",
        "\n",
        "# Reset the agent weights when the option is selected or when the agent is not\n",
        "# defined yet - This initializes the network.\n",
        "if 'my_trained_agent' not in locals() or reset_agent_weights:\n",
        "  inputs, outputs = config['model'](config['model_config'])\n",
        "  my_trained_agent = Model(inputs=inputs, outputs=outputs)\n",
        "if plot_agent_architecture:\n",
        "  plot_model(my_trained_agent, show_shapes=True, show_layer_names=True,\n",
        "             to_file='model.png')\n",
        "  display(Image(retina=True, filename='model.png'))\n",
        "\n",
        "# Clear the experience buffer when the option is selected or when the buffer is\n",
        "# not defined yet.\n",
        "if 'experience_buffer' not in locals() or reset_experience_buffer:\n",
        "  experience_buffer = ExperienceBuffer(config['max_experience_buffer_games'])\n",
        "\n",
        "while True:\n",
        "  # 1) Add self-play experience to the experience replay buffer.\n",
        "  experience = self_play(my_trained_agent, \n",
        "                         config['num_self_play_games_per_iteration'],\n",
        "                         config['verbose_self_play'],\n",
        "                         config['epsilon_greedy_parameter'],\n",
        "                         )\n",
        "  experience_buffer.add(experience)\n",
        "\n",
        "  # 2) Update the Q network using the experience from the experience buffer\n",
        "  print(\"Experience buffer size: {} transitions ({} episodes)\".format(\n",
        "      experience_buffer.size(), experience_buffer.num_episodes()))\n",
        "  for _ in range(config['num_learning_updates_per_iteration']):\n",
        "    update_agent(experience_buffer.get_all_data(), my_trained_agent,\n",
        "                config['learning_config'])\n",
        "  \n",
        "  # 3) Evaluate the greedy trained agent against a random agent to see if we are\n",
        "  #    making progress\n",
        "  mean_eval_reward = eval_greedy_versus_random(\n",
        "      my_trained_agent, config['num_evaluation_games'])\n",
        "  print(\"Mean reward against random agent in {} games: {}\".format(\n",
        "      config['num_evaluation_games'], mean_eval_reward))\n",
        "  print(\"\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "image/png": {
              "height": 313,
              "width": 218
            }
          },
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start move Q-values: [0.435 0.398 0.457 0.326 0.391 0.457 0.371]\n",
            "Experience buffer size: 30600 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.4\n",
            "\n",
            "Start move Q-values: [0.451 0.274 0.483 0.312 0.291 0.46  0.374]\n",
            "Experience buffer size: 29600 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.4\n",
            "\n",
            "Start move Q-values: [0.46  0.196 0.489 0.229 0.36  0.431 0.399]\n",
            "Experience buffer size: 27400 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.6\n",
            "\n",
            "Start move Q-values: [0.404 0.271 0.577 0.161 0.422 0.424 0.359]\n",
            "Experience buffer size: 28400 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.64\n",
            "\n",
            "Start move Q-values: [0.335 0.324 0.442 0.123 0.317 0.39  0.486]\n",
            "Experience buffer size: 31000 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.2\n",
            "\n",
            "Start move Q-values: [0.449 0.302 0.451 0.155 0.441 0.423 0.409]\n",
            "Experience buffer size: 32200 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.16\n",
            "\n",
            "Start move Q-values: [0.409 0.163 0.502 0.159 0.458 0.366 0.469]\n",
            "Experience buffer size: 33800 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.44\n",
            "\n",
            "Start move Q-values: [0.479 0.154 0.466 0.135 0.417 0.421 0.454]\n",
            "Experience buffer size: 33400 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.52\n",
            "\n",
            "Start move Q-values: [0.467 0.103 0.499 0.146 0.427 0.454 0.384]\n",
            "Experience buffer size: 33800 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.68\n",
            "\n",
            "Start move Q-values: [0.36  0.098 0.488 0.147 0.353 0.448 0.441]\n",
            "Experience buffer size: 33000 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.68\n",
            "\n",
            "Start move Q-values: [0.409 0.09  0.552 0.182 0.446 0.418 0.403]\n",
            "Experience buffer size: 30800 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.32\n",
            "\n",
            "Start move Q-values: [0.394 0.092 0.487 0.136 0.407 0.429 0.364]\n",
            "Experience buffer size: 32600 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.28\n",
            "\n",
            "Start move Q-values: [0.509 0.059 0.514 0.115 0.477 0.389 0.355]\n",
            "Experience buffer size: 33000 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.4\n",
            "\n",
            "Start move Q-values: [0.503 0.091 0.605 0.152 0.51  0.44  0.32 ]\n",
            "Experience buffer size: 35800 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.28\n",
            "\n",
            "Start move Q-values: [0.43  0.097 0.629 0.132 0.566 0.446 0.26 ]\n",
            "Experience buffer size: 33600 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.48\n",
            "\n",
            "Start move Q-values: [0.394 0.199 0.655 0.167 0.428 0.388 0.257]\n",
            "Experience buffer size: 33200 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.56\n",
            "\n",
            "Start move Q-values: [0.187 0.147 0.637 0.21  0.548 0.453 0.431]\n",
            "Experience buffer size: 31600 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.56\n",
            "\n",
            "Start move Q-values: [0.358 0.197 0.578 0.141 0.392 0.407 0.532]\n",
            "Experience buffer size: 31600 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.56\n",
            "\n",
            "Start move Q-values: [0.188 0.131 0.502 0.241 0.461 0.426 0.423]\n",
            "Experience buffer size: 33800 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.56\n",
            "\n",
            "Start move Q-values: [0.226 0.196 0.503 0.264 0.694 0.33  0.44 ]\n",
            "Experience buffer size: 35400 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.76\n",
            "\n",
            "Start move Q-values: [0.16  0.155 0.47  0.289 0.298 0.327 0.284]\n",
            "Experience buffer size: 37600 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.72\n",
            "\n",
            "Start move Q-values: [0.132 0.099 0.561 0.46  0.171 0.266 0.162]\n",
            "Experience buffer size: 37600 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.72\n",
            "\n",
            "Start move Q-values: [0.084 0.076 0.588 0.417 0.099 0.254 0.166]\n",
            "Experience buffer size: 37600 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.72\n",
            "\n",
            "Start move Q-values: [0.069 0.064 0.718 0.291 0.098 0.301 0.161]\n",
            "Experience buffer size: 34000 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.6\n",
            "\n",
            "Start move Q-values: [0.174 0.05  0.812 0.234 0.103 0.485 0.09 ]\n",
            "Experience buffer size: 33400 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.52\n",
            "\n",
            "Start move Q-values: [0.164 0.065 0.791 0.203 0.105 0.424 0.112]\n",
            "Experience buffer size: 35000 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.64\n",
            "\n",
            "Start move Q-values: [0.06  0.048 0.724 0.215 0.118 0.482 0.09 ]\n",
            "Experience buffer size: 34400 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.52\n",
            "\n",
            "Start move Q-values: [0.085 0.042 0.668 0.198 0.147 0.432 0.172]\n",
            "Experience buffer size: 33200 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.28\n",
            "\n",
            "Start move Q-values: [0.078 0.053 0.585 0.262 0.278 0.513 0.071]\n",
            "Experience buffer size: 31800 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.44\n",
            "\n",
            "Start move Q-values: [0.127 0.042 0.608 0.255 0.299 0.571 0.05 ]\n",
            "Experience buffer size: 31600 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.52\n",
            "\n",
            "Start move Q-values: [0.11  0.04  0.656 0.227 0.374 0.765 0.054]\n",
            "Experience buffer size: 29600 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.68\n",
            "\n",
            "Start move Q-values: [0.176 0.026 0.641 0.203 0.386 0.371 0.06 ]\n",
            "Experience buffer size: 29200 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.76\n",
            "\n",
            "Start move Q-values: [0.025 0.017 0.609 0.144 0.289 0.314 0.045]\n",
            "Experience buffer size: 29800 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.44\n",
            "\n",
            "Start move Q-values: [0.009 0.009 0.653 0.28  0.23  0.33  0.045]\n",
            "Experience buffer size: 29400 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.4\n",
            "\n",
            "Start move Q-values: [0.019 0.031 0.747 0.416 0.124 0.364 0.109]\n",
            "Experience buffer size: 29400 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.56\n",
            "\n",
            "Start move Q-values: [0.011 0.035 0.806 0.501 0.103 0.397 0.074]\n",
            "Experience buffer size: 27000 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.68\n",
            "\n",
            "Start move Q-values: [0.007 0.039 0.84  0.538 0.096 0.398 0.068]\n",
            "Experience buffer size: 26000 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.68\n",
            "\n",
            "Start move Q-values: [0.018 0.076 0.759 0.386 0.241 0.421 0.08 ]\n",
            "Experience buffer size: 26000 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.52\n",
            "\n",
            "Start move Q-values: [0.011 0.053 0.747 0.366 0.243 0.249 0.069]\n",
            "Experience buffer size: 25800 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.76\n",
            "\n",
            "Start move Q-values: [0.038 0.063 0.843 0.449 0.693 0.242 0.115]\n",
            "Experience buffer size: 25000 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.36\n",
            "\n",
            "Start move Q-values: [0.043 0.01  0.956 0.241 0.907 0.176 0.024]\n",
            "Experience buffer size: 25200 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.76\n",
            "\n",
            "Start move Q-values: [0.005 0.006 0.802 0.222 0.952 0.084 0.025]\n",
            "Experience buffer size: 23200 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.6\n",
            "\n",
            "Start move Q-values: [0.002 0.002 0.891 0.15  0.655 0.133 0.018]\n",
            "Experience buffer size: 25800 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.56\n",
            "\n",
            "Start move Q-values: [0.001 0.006 0.649 0.372 0.301 0.089 0.012]\n",
            "Experience buffer size: 27400 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.56\n",
            "\n",
            "Start move Q-values: [0.003 0.004 0.81  0.327 0.464 0.198 0.001]\n",
            "Experience buffer size: 27600 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.44\n",
            "\n",
            "Start move Q-values: [0.003 0.008 0.731 0.293 0.532 0.323 0.   ]\n",
            "Experience buffer size: 28600 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.6\n",
            "\n",
            "Start move Q-values: [0.003 0.007 0.886 0.59  0.483 0.228 0.001]\n",
            "Experience buffer size: 30200 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.64\n",
            "\n",
            "Start move Q-values: [0.002 0.004 0.851 0.724 0.274 0.229 0.001]\n",
            "Experience buffer size: 30800 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.56\n",
            "\n",
            "Start move Q-values: [0.002 0.002 0.947 0.765 0.258 0.628 0.002]\n",
            "Experience buffer size: 30400 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.6\n",
            "\n",
            "Start move Q-values: [0.001 0.003 0.85  0.589 0.431 0.579 0.003]\n",
            "Experience buffer size: 31600 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.68\n",
            "\n",
            "Start move Q-values: [0.001 0.002 0.898 0.428 0.307 0.696 0.002]\n",
            "Experience buffer size: 35692 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.52\n",
            "\n",
            "Start move Q-values: [0.    0.01  0.712 0.442 0.288 0.329 0.001]\n",
            "Experience buffer size: 36892 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.56\n",
            "\n",
            "Start move Q-values: [0.    0.004 0.695 0.572 0.397 0.529 0.001]\n",
            "Experience buffer size: 33292 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.68\n",
            "\n",
            "Start move Q-values: [0.    0.003 0.584 0.284 0.329 0.375 0.001]\n",
            "Experience buffer size: 34492 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.64\n",
            "\n",
            "Start move Q-values: [0.    0.004 0.432 0.309 0.144 0.231 0.   ]\n",
            "Experience buffer size: 33892 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.52\n",
            "\n",
            "Start move Q-values: [0.    0.012 0.343 0.268 0.09  0.37  0.001]\n",
            "Experience buffer size: 33292 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.4\n",
            "\n",
            "Start move Q-values: [0.    0.046 0.25  0.146 0.149 0.825 0.   ]\n",
            "Experience buffer size: 32492 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.48\n",
            "\n",
            "Start move Q-values: [0.    0.03  0.377 0.112 0.067 0.727 0.   ]\n",
            "Experience buffer size: 33492 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.72\n",
            "\n",
            "Start move Q-values: [0.    0.111 0.416 0.057 0.073 0.778 0.001]\n",
            "Experience buffer size: 32692 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.6\n",
            "\n",
            "Start move Q-values: [0.    0.053 0.645 0.021 0.169 0.236 0.   ]\n",
            "Experience buffer size: 31692 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.44\n",
            "\n",
            "Start move Q-values: [0.    0.113 0.517 0.094 0.118 0.659 0.002]\n",
            "Experience buffer size: 27000 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.48\n",
            "\n",
            "Start move Q-values: [0.    0.349 0.765 0.118 0.238 0.493 0.001]\n",
            "Experience buffer size: 25200 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.64\n",
            "\n",
            "Start move Q-values: [0.    0.755 0.476 0.101 0.417 0.264 0.001]\n",
            "Experience buffer size: 25400 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.48\n",
            "\n",
            "Start move Q-values: [0.    0.888 0.045 0.038 0.423 0.231 0.005]\n",
            "Experience buffer size: 23200 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.6\n",
            "\n",
            "Start move Q-values: [0.    0.895 0.078 0.032 0.181 0.148 0.438]\n",
            "Experience buffer size: 23200 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.64\n",
            "\n",
            "Start move Q-values: [0.    0.832 0.489 0.066 0.329 0.429 0.035]\n",
            "Experience buffer size: 27600 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.6\n",
            "\n",
            "Start move Q-values: [0.    0.916 0.187 0.176 0.247 0.132 0.892]\n",
            "Experience buffer size: 27600 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.44\n",
            "\n",
            "Start move Q-values: [0.    0.839 0.465 0.202 0.525 0.507 0.127]\n",
            "Experience buffer size: 26200 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.56\n",
            "\n",
            "Start move Q-values: [0.001 0.957 0.38  0.081 0.482 0.585 0.073]\n",
            "Experience buffer size: 25800 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.36\n",
            "\n",
            "Start move Q-values: [0.001 0.977 0.174 0.025 0.783 0.874 0.015]\n",
            "Experience buffer size: 25400 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.32\n",
            "\n",
            "Start move Q-values: [0.002 0.959 0.356 0.016 0.833 0.824 0.027]\n",
            "Experience buffer size: 25800 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.4\n",
            "\n",
            "Start move Q-values: [0.005 0.879 0.345 0.017 0.644 0.843 0.089]\n",
            "Experience buffer size: 26400 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.56\n",
            "\n",
            "Start move Q-values: [0.01  0.929 0.165 0.023 0.932 0.913 0.394]\n",
            "Experience buffer size: 27200 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.6\n",
            "\n",
            "Start move Q-values: [0.007 0.957 0.361 0.039 0.612 0.714 0.628]\n",
            "Experience buffer size: 28200 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.56\n",
            "\n",
            "Start move Q-values: [0.    0.937 0.005 0.002 0.221 0.8   0.662]\n",
            "Experience buffer size: 30665 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.6\n",
            "\n",
            "Start move Q-values: [0.001 0.968 0.025 0.    0.335 0.941 0.074]\n",
            "Experience buffer size: 27281 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.48\n",
            "\n",
            "Start move Q-values: [0.001 0.962 0.091 0.    0.158 0.99  0.009]\n",
            "Experience buffer size: 27481 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.6\n",
            "\n",
            "Start move Q-values: [0.    0.983 0.005 0.    0.382 0.266 0.024]\n",
            "Experience buffer size: 27081 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.48\n",
            "\n",
            "Start move Q-values: [0.    0.971 0.003 0.    0.065 0.366 0.035]\n",
            "Experience buffer size: 28281 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.56\n",
            "\n",
            "Start move Q-values: [0.    0.079 0.001 0.    0.017 0.127 0.001]\n",
            "Experience buffer size: 29681 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.68\n",
            "\n",
            "Start move Q-values: [0.    0.348 0.    0.    0.024 0.073 0.01 ]\n",
            "Experience buffer size: 29081 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.36\n",
            "\n",
            "Start move Q-values: [0.    0.266 0.003 0.    0.072 0.089 0.001]\n",
            "Experience buffer size: 28881 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.32\n",
            "\n",
            "Start move Q-values: [0.    0.706 0.005 0.    0.014 0.077 0.006]\n",
            "Experience buffer size: 32278 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.32\n",
            "\n",
            "Start move Q-values: [0.    0.328 0.001 0.    0.003 0.079 0.021]\n",
            "Experience buffer size: 31378 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.52\n",
            "\n",
            "Start move Q-values: [0.    0.851 0.001 0.    0.003 0.064 0.007]\n",
            "Experience buffer size: 29113 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.32\n",
            "\n",
            "Start move Q-values: [0.    0.704 0.013 0.001 0.001 0.044 0.007]\n",
            "Experience buffer size: 28497 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.56\n",
            "\n",
            "Start move Q-values: [0.    0.902 0.057 0.001 0.011 0.098 0.005]\n",
            "Experience buffer size: 31097 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.32\n",
            "\n",
            "Start move Q-values: [0.    0.989 0.01  0.001 0.021 0.033 0.034]\n",
            "Experience buffer size: 32097 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.4\n",
            "\n",
            "Start move Q-values: [0.    0.974 0.021 0.    0.017 0.01  0.007]\n",
            "Experience buffer size: 32497 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.4\n",
            "\n",
            "Start move Q-values: [0.    0.99  0.03  0.    0.009 0.008 0.007]\n",
            "Experience buffer size: 32097 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.56\n",
            "\n",
            "Start move Q-values: [0.    0.995 0.118 0.    0.003 0.024 0.002]\n",
            "Experience buffer size: 34753 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.52\n",
            "\n",
            "Start move Q-values: [0.    0.964 0.133 0.001 0.002 0.028 0.002]\n",
            "Experience buffer size: 35153 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.32\n",
            "\n",
            "Start move Q-values: [0.    0.998 0.349 0.002 0.004 0.1   0.001]\n",
            "Experience buffer size: 31453 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.44\n",
            "\n",
            "Start move Q-values: [0.    0.996 0.415 0.004 0.02  0.3   0.005]\n",
            "Experience buffer size: 32553 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.44\n",
            "\n",
            "Start move Q-values: [0.    0.997 0.442 0.003 0.009 0.46  0.001]\n",
            "Experience buffer size: 35803 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.52\n",
            "\n",
            "Start move Q-values: [0.    0.998 0.211 0.001 0.004 0.179 0.   ]\n",
            "Experience buffer size: 39003 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.56\n",
            "\n",
            "Start move Q-values: [0.    0.999 0.278 0.001 0.001 0.118 0.005]\n",
            "Experience buffer size: 38003 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.56\n",
            "\n",
            "Start move Q-values: [0.    1.    0.141 0.011 0.001 0.117 0.036]\n",
            "Experience buffer size: 36803 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.52\n",
            "\n",
            "Start move Q-values: [0.    0.999 0.623 0.02  0.002 0.219 0.005]\n",
            "Experience buffer size: 39360 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.56\n",
            "\n",
            "Start move Q-values: [0.    0.999 0.279 0.006 0.    0.24  0.016]\n",
            "Experience buffer size: 38560 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: -0.04\n",
            "\n",
            "Start move Q-values: [0.    0.999 0.677 0.013 0.    0.538 0.011]\n",
            "Experience buffer size: 38329 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.32\n",
            "\n",
            "Start move Q-values: [0.    0.997 0.289 0.001 0.    0.392 0.   ]\n",
            "Experience buffer size: 39657 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.28\n",
            "\n",
            "Start move Q-values: [0.    0.999 0.178 0.001 0.    0.329 0.   ]\n",
            "Experience buffer size: 40960 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.28\n",
            "\n",
            "Start move Q-values: [0.    1.    0.128 0.001 0.    0.965 0.   ]\n",
            "Experience buffer size: 40360 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.12\n",
            "\n",
            "Start move Q-values: [0.    0.991 0.025 0.    0.    0.933 0.   ]\n",
            "Experience buffer size: 36910 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.4\n",
            "\n",
            "Start move Q-values: [0.    0.817 0.004 0.    0.    0.924 0.   ]\n",
            "Experience buffer size: 33510 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.56\n",
            "\n",
            "Start move Q-values: [0.    0.935 0.002 0.    0.    0.042 0.   ]\n",
            "Experience buffer size: 31710 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.56\n",
            "\n",
            "Start move Q-values: [0.    0.472 0.003 0.    0.    0.022 0.   ]\n",
            "Experience buffer size: 31110 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.32\n",
            "\n",
            "Start move Q-values: [0.    0.277 0.002 0.    0.    0.031 0.   ]\n",
            "Experience buffer size: 26353 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.36\n",
            "\n",
            "Start move Q-values: [0.    0.89  0.001 0.    0.    0.016 0.   ]\n",
            "Experience buffer size: 29353 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.44\n",
            "\n",
            "Start move Q-values: [0.    0.804 0.    0.    0.    0.005 0.   ]\n",
            "Experience buffer size: 30528 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.4\n",
            "\n",
            "Start move Q-values: [0.    0.666 0.001 0.    0.    0.01  0.   ]\n",
            "Experience buffer size: 30200 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.28\n",
            "\n",
            "Start move Q-values: [0.    0.995 0.    0.    0.    0.012 0.   ]\n",
            "Experience buffer size: 31000 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.16\n",
            "\n",
            "Start move Q-values: [0.    0.965 0.    0.    0.    0.004 0.   ]\n",
            "Experience buffer size: 35762 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.36\n",
            "\n",
            "Start move Q-values: [0.    0.858 0.    0.    0.    0.003 0.   ]\n",
            "Experience buffer size: 36762 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.48\n",
            "\n",
            "Start move Q-values: [0.    0.996 0.    0.    0.    0.002 0.   ]\n",
            "Experience buffer size: 38558 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.4\n",
            "\n",
            "Start move Q-values: [0.    0.999 0.001 0.    0.    0.012 0.   ]\n",
            "Experience buffer size: 39758 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.44\n",
            "\n",
            "Start move Q-values: [0.    0.996 0.003 0.    0.    0.008 0.   ]\n",
            "Experience buffer size: 42158 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.44\n",
            "\n",
            "Start move Q-values: [0.    1.    0.002 0.    0.    0.001 0.003]\n",
            "Experience buffer size: 44158 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.4\n",
            "\n",
            "Start move Q-values: [0.    1.    0.003 0.    0.    0.005 0.039]\n",
            "Experience buffer size: 41227 transitions (2000 episodes)\n",
            "Mean reward against random agent in 50 games: 0.16\n",
            "\n",
            "Start move Q-values: [0.    0.991 0.001 0.    0.    0.    0.004]\n",
            "Experience buffer size: 38635 transitions (2000 episodes)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKkDXpHVXwoy"
      },
      "source": [
        "## Let's play against our trained agent to see if we can outsmart it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ilfu6xpQXvlD"
      },
      "source": [
        "play_against_trained = True #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "plot_resolution = 400 #@param {type:\"slider\", min:200, max:500, step:1}\n",
        "print(my_trained_agent.predict(np.ones((1, 6, 7, 3))))\n",
        "\n",
        "# Here we define an agent that picks the best action according to the trained\n",
        "# Q-value network. There is a lot of code duplication with play_against_agent\n",
        "# becaus I ran into issues making the trained agent accessible to the Graph.\n",
        "# Always play as first position against the opposing agent.\n",
        "# Adapted from https://www.kaggle.com/marcovasquez/how-to-play-with-computer-and-check-winner\n",
        "def play_against_trained_agent(agent):\n",
        "  env = make_game(\"connectx\", debug=False, configuration={\"timeout\": 10})\n",
        "\n",
        "  human_starts = np.random.uniform() > 0.5\n",
        "  human_id = int(not human_starts)\n",
        "  human_move = human_starts\n",
        "  while not env.done:\n",
        "    if human_move:\n",
        "      clear_output(wait=True) # Comment if you want to keep track of every action\n",
        "      print(\"{}'s color: {}\".format(\n",
        "          your_name, \"Blue\" if human_starts else \"Grey\"))\n",
        "      env.render(mode=\"ipython\", width=plot_resolution, height=plot_resolution,\n",
        "                header=False, controls=False)\n",
        "      observation = env.state[human_id].observation\n",
        "\n",
        "      # Plot the opponent q-values when they are available\n",
        "      if (np.array(observation.board) == 1).sum() > 0:\n",
        "        print(\"Previous step agent Q-values: {}\".format(np.round(q_values, 2)))\n",
        "\n",
        "      action = get_input(your_name, observation, env.configuration)\n",
        "      if action is None:\n",
        "        print(\"Exiting game after pressing q\")\n",
        "        return\n",
        "      env.step([int(action), None] if human_starts else [None, int(action)])\n",
        "    else:\n",
        "      current_network_input = obs_to_network_input(\n",
        "          env.state[1-human_id].observation, env.configuration,\n",
        "          player_id=1-human_id)\n",
        "      # Take the greedy action of the agent\n",
        "      q_values, action = get_agent_q_and_a(\n",
        "          agent, current_network_input, epsilon_greedy_parameter=0)\n",
        "      env.step([None, int(action)] if human_starts else [int(action), None])\n",
        "    human_move = not human_move\n",
        "\n",
        "    observation = env.state[human_id].observation\n",
        "    if (check_winner(observation) == (human_id+1)):\n",
        "      print(\"You Won, Amazing! \\nGAME OVER\")\n",
        "        \n",
        "    elif (check_winner(observation) == (2-human_id)):\n",
        "      print(\"The agent Won! \\nGAME OVER\")\n",
        "\n",
        "  if (check_winner(observation) is None):\n",
        "    print(\"That is a draw between you and the agent\")\n",
        "\n",
        "  env.render(mode=\"ipython\", width=plot_resolution, height=plot_resolution,\n",
        "            header=False, controls=False)\n",
        "    \n",
        "if play_against_trained:\n",
        "  play_against_trained_agent(my_trained_agent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxvLp0ESkHx4"
      },
      "source": [
        "## Advanced - suggestions on how to make your agent stronger and let the genie out of the bottle.\n",
        "\n",
        "*   Use a [convolutional network](https://https://keras.io/examples/mnist_cnn/) instead of a multi-layer perceptron (logic already provided - you only need to change the 'model' setting) - experiment with what architecture works best.\n",
        "*   Use N-step returns (N=20, lambda=0.9 is a good start setting) instead of one step returns (logic already provided - you only need to change the 'return_steps_trace' setting).\n",
        "*   Make use of the vertical symmetry of the game - double the learning experience and make the acting more consistent.\n",
        "*   More directed exploration - e.g. [Boltzmann exploration](https://automaticaddison.com/boltzmann-distribution-and-epsilon-greedy-search/)\n",
        "*   Evaluate against the previous iteration: define a new agent iteration if we confidently beat the previous iteration (e.g. >= 70% of the rewards)\n",
        "*   Add the evaluation experience to the experience replay buffer - all experience is valuable since the environment interaction is expensive!\n",
        "*   Checkpoint an agent when it is significantly better than the current iteration and play against all previous and the current checkpoint in self-play\n",
        "*   Override the Q-values of actions that lead to a certain win or loss under perfect play of the opponent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQJl28XOHYd5"
      },
      "source": [
        "# Intentionally blank - cheat sheet below\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0LVtnF3rKXn"
      },
      "source": [
        "## Cheat sheet: intentional bugs and obvious areas for improvements in the original code\n",
        "####Obvious bugs:\n",
        "*   Bug in encoding of who is playing - **obs_to_network_input** - off by one in player id. Root cause: spending too much time in R. The result is that the network does not know who is playing. Fix:\n",
        "```\n",
        "def obs_to_network_input(observation, configuration, player_id):\n",
        "      board = np.array(observation.board).reshape(\n",
        "        configuration.rows, configuration.columns)\n",
        "      \n",
        "      # One hot encoding of the inputs (empty, player 1, player 2)\n",
        "      obs_input = (np.arange(3) == board[..., None]).astype(float)\n",
        "      \n",
        "      # Swap player 1 and player 2 positions? The network always assumes that 'my'\n",
        "      # stones (player_id) come first\n",
        "      if player_id == 1: # <---------------------\n",
        "          tmp = obs_input[:, :, 1].copy()\n",
        "          obs_input[:, :, 1] = obs_input[:, :, 2]\n",
        "          obs_input[:, :, 2] = tmp\n",
        "      \n",
        "      return obs_input\n",
        "```\n",
        "*   Not using the true reward signal - in **one_step_minimax_q_targets** - Change to:\n",
        "\n",
        "```\n",
        "target_qs = np.array([t if l else n for(t, l, n) in zip(\n",
        "    terminal_rewards, last_episode_actions, next_q_minimax_star)])\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "####Obvious areas for improvement:\n",
        "*   Not exploring when collecting experience - set epsilon greedy to something like 0.2 (anything but 0, really) - decay as training progresses according to theory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_ALc2o0kGms"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}